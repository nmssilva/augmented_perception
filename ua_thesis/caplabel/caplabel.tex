\chapter{Object Detection, Tracking and Labelling}

This chapter explains how the object detection, tracking and labelling application was implemented. First, it will be described how this is performed in the 2D image. 

Secondly, the implementation of the tracking of targets using \gls{lidar} sensors will be explained. Lastly, the combination of the 2D image and the \gls{lidar} data is explained.

The image sequences and laser scan data obtained for the development of this stage of the dissertation were recorded into rosbags using the ATLASCAR 2 sensors.

\section{Image Tracking}

The development of object detection, tracking and labelling starts by processing and analyzing the image sequences. A labelling node was created in \gls{ros} where the features in this chapter were implemented. 

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=0.7\textwidth]{caplabel/imgs/templatediagram.pdf}
	
	\caption{Image tracking algorithm diagram}
	\label{fig:imagediagram}
	
\end{figure}

Figure \ref{fig:imagediagram} presents an overview of the image tracking algorithm. It starts by receiving frames and by selecting a target on the images. Finally, template matching is applied to follow the object.

To obtain the image frames, this node subscribes the camera images through its rostopic. The image is converted from the \gls{ros} message format into an \gls{opencv} format so it can be easily manipulated. \gls{opencv} treats images as matrices of pixel with $(x,y)$ coordinates and RGB values. As the image sequences arrive, they are stored into a queue. This queue will be used later to look back to the previous frames and back-track the object. 

When the node starts, a window opens (see figure \ref{fig:view}) for the user to view the video stream recorded in the bag. The node also functions in real-time. In other words, the node can be executed by connecting the computer directly to the car, obtaining the images in immediately. In this window, the user can click on objects that may appear. 


\begin{figure}[htp]
	
	\centering
	\includegraphics[width=0.8\textwidth]{caplabel/imgs/view.png}
	
	\caption{Window view with image sequences appearing}
	\label{fig:view}
	
\end{figure}

When the user clicks on an object, a callback function is triggered to process the mouse event and a bounding box with a predefined size appears around the click position.

During this process, when the image is iterated the upper third-part of the image is ignored as it is considered to be irrelevant content as most of it will be tall objects like trees or objects in the sky, like clouds.

The bounding box follows the selected target. To accomplish this, template matching techniques are used. Firstly, the previous frames are saved. The node will check the queue of previous frames and store them to use them later. 

The template matching strategy is used to track the selected target in the next frames. It begins by copying the source image to display to another \gls{opencv} matrix and also creates a result matrix. The matching is now performed using a method implemented by \gls{opencv} called \texttt{matchTemplate} which takes the source image, the patch (which is the \gls{roi} inside the bounding box), the result matrix and a matching method. 

Template matching is a technique for finding areas of an image that match (are similar) to a template image (patch). A resulting image is calculated by iterating the source image and comparing the template with the area in that position (see figure \ref{fig:resultmat}). For each position a score is assigned representing how good the match is in that position.

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=0.49\textwidth]{caplabel/imgs/resultmat.png}\hfill
	\includegraphics[width=0.49\textwidth]{caplabel/imgs/resultmat2.png}
	
	\caption{Template matching example result matrix}
	\label{fig:resultmat}
	
\end{figure}

It is important to note that after each frame is received, the patch used for the next template matching cycle will be the \gls{roi} acquired in the previous frame. This means that the patch is updated when a new frame is received to obtain better accuracy of the object's next pose. 

This step concludes the front tracking. It is important to notice that this application features tracking of the objects not only for the next frames but also for the previous. The tracking for the next and previous frames is respectively the front and back tracking.

After the whole tracking is done, the back tracking is now performed. The reason why the back tracking is done after the front tracking is mainly because of processing times. If the back tracking began when a target is selected in the image before the front tracking, some time would be wasted to process the previous frames, losing some of the next frames used for the front tracking. 

With a queue, the last frames are saved and at the moment of object selection those frames are copied and saved to be processed posteriorly. After the front tracking, the node gets the frames stored before the target selection and applies template matching. With this, the tracking is done in both directions. In figure \ref{fig:tracking} an example of the tracking is presented.


\begin{figure}[htp]
	
	\centering
	\includegraphics[width=.44\textwidth]{caplabel/imgs/f1}
	\includegraphics[width=.44\textwidth]{caplabel/imgs/f2}
	
	
	\includegraphics[width=.44\textwidth]{caplabel/imgs/f3}
	
	
	\includegraphics[width=.44\textwidth]{caplabel/imgs/f4}
	\includegraphics[width=.44\textwidth]{caplabel/imgs/f5}
	
	\caption{Example of back tracking and front tracking: the picture in the middle is the selected frame, the two upper frames show the back tracking and the two lower frames show to front tracking.}
	\label{fig:tracking}
	
\end{figure}

\section{Range Based Tracking}

To improve the tracking, the image process is combined using the \gls{lidar} scanners in the ATLASCAR 2. To develop this part, a continuation to the previous labelling node is added where the capabilities of the laser scans will be explored. 


\begin{figure}
	
	\centering
	\includegraphics[width=0.9\textwidth]{caplabel/imgs/mttdiagram.pdf}
	
	\caption{Range Based Detector Algorithm Diagram}
	\label{fig:mttdiagram}
	
\end{figure}

In figure \ref{fig:mttdiagram} an overview of the range based detector algorithm is presented. It starts by receiving data from the LIDAR sensors and converting it to a pointcloud object. Finally, the detection is done by clustering the pointcloud where each cluster will represent a detected object. The clustering is performed with the \gls{mtt} library developed by \cite{SoaresDeAlmeida2016a}. 

The \gls{mtt} works with planar scanners to obtain perception although it receives a pointcloud as input. The \gls{mtt} library supposes that the objects are all at the same height so the pointclouds are flattened. For the scope of this project this is no problem as most of the scanners used are planar except for the SICK LD-MRS. Assuming that the readings of this \gls{lidar} are at the same height does not influence the results as the difference of the measures are minimal. 

The node starts by subscribing to all topics where \texttt{laserScans} can be found. There are two SICK LMS151, one on each side of the ATLASCAR 2, giving two planar scans with a 270 degree aperture. The SICK LD-MRS features four planar scans. Each scan is submitted into a ROS topic totaling six topics, one for each SICK LMS151 and four topics for the SICK LD-MRS. The six topics are subscribed and the sensor messages are given to a callback function in the \gls{ros} format.

This callback functions gets the laser frame ID. This frame is the transformation frame, not to be misunderstood with an image frame. The frame ID is used to identify the laser scan in the callback function. The callback function converts the \texttt{laserScan} in to a pointcloud.

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=0.6\textwidth]{caplabel/imgs/rviz0.png}
	
	\caption{All the separated laserScans visualized with Rviz}
	\label{fig:rviz0}
	
\end{figure}

In figure \ref{fig:rviz0} the \texttt{laserScans} can be observed individually. The readings from the central SICK LD-MRS are given by the white points. The two SICK LMS151 are distinguished by the red and green colors for the left and right respectively. The colors are chosen regarding nautical and aeronautical navigation lights for port and starboard positions.

In the image callback function, when an image frame arrives it creates a full pointcloud by merging the pointclouds of all \texttt{laserScans}. To do this, a transform listener is created to calculate the transforms at that given time between the two SICK LMS151 and the SICK LD-MRS. The \gls{pcl} library is used here to blend the different \texttt{laserScans}. The \gls{pcl} can concatenate pointclouds making it easy to merge them all together.

Finally, the pointcloud is filtered to a squared area in front of the car in order to avoid unwanted objects to appear such as roadsides on the highway. The pointclouds are converted from the \gls{ros} format to \gls{pcl} format, concatenated, and ready to be processed by the \gls{mtt} library. 

The next step is to cluster the different objects found in the pointcloud. The clustering strategy is implemented by the \gls{mtt} library and it is based on the Nearest Neighbor Clustering algorithm (\cite{Yu2018}). The \gls{mtt} takes the full data and initializes a vector of clusters. Clusters are formed by points and its structure contains an id of the cluster, start and end points, total number of points, length of the cluster, among other variables and flags used for occlusion detection. If the distance between points is larger than a given threshold, then a cluster is formed and an object is most likely to be there.

To make it easier to visualize, markers are created and placed in the location of the objects. For each object in the target list, a marker is created with the ID of the object. The ID increments by one as a new object is found. 

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=0.65\textwidth]{caplabel/imgs/rviz1.png}
	
	\caption{Visualization of a detected car with \gls{mtt} in Rviz}
	\label{fig:rviz1}
	
\end{figure}

In the position of the object, a visual marker is placed. There are also markers in the form of line strips in order to visualize the connection between the points of the pointcloud. An addition to the marker creation method was made, where 3D bounding boxes with a predefined static size are created in the location of the objects.

The information in figure \ref{fig:rviz1} was visualized using the Rviz tool. The \gls{mtt} creates by default a marker at the origin where usually the front of the ATLASCAR 2 is (depending on the transformations). The processed pointcloud can be seen, where the \gls{lidar} \texttt{laserScans} messages are all merged and a green 3D bounding box with the ID 103 is shown meaning that an object was found at that location. The object was in fact a car traveling in front of the ATLASCAR 2. The roadsides are not detected since the pointcloud was filtered to a squared area in front of the car in order to avoid objects with no interest. Only part of the pointcloud in figure \ref{fig:rviz1} is processed by the \gls{mtt}.

\section{Sensor Data Fusion}

To accomplish sensor data fusion, a multi-modal approach was utilized, combining data retrieved from several ranged based and visual sensors.

Since the detection, tracking and labelling can be done independently with the 2D image and with the \gls{lidar} data, this section will explain how the combination of both can be done.

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=0.99\textwidth]{caplabel/imgs/multimodal}
	
	\caption{Overview of the multi-modal approach. }
	\label{fig:multimodal}
	
\end{figure}

Figure \ref{fig:multimodal} depicts the process of the multi-modal approach. In sensor fusion, ranged based information is combined with the image data allowing augmented perception of the surroundings. With the ability to have basic cognition of the environment, it is possible to detect and track objects in motion (\cite{Spinello2010}).

\subsection{Dynamic 2D Bounding Box Size}

Previously, the bounding box size was static but using the 3D position given by the \gls{mtt} it is possible to implement dynamic bounding box size.

The 2D bounding box size is given by the distance to the object. If the distance is greater, the bounding box will be smaller and vice-versa. The distance to the object is given by the tri-dimensional spacial coordinates given by the \gls{mtt} algorithm.

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=0.45\textwidth]{caplabel/imgs/boxds1.png}
	\includegraphics[width=0.45\textwidth]{caplabel/imgs/boxds2.png}
	
	\caption{Example of near (left) and far (right) car with dynamic bounding box size}
	\label{fig:dsize}
	
\end{figure}

Figure \ref{fig:dsize} depicts an example of the dynamic bounding box as the target moves away.

\subsection{Pointcloud Projection}

By combining the sensor data with the image it is possible to check where the pointcloud is relatively to the camera position. To accomplish this it is needed to firstly make the intrinsic calibration of the camera, and then proceed to the implementation of the pointcloud projection. By using a \gls{ros} node in the package \texttt{camera\_calibration}, the intrinsic values of the camera and the distortion coefficients are obtained (see listing \ref{lst: intcalib}).

\begin{figure}
	\begin{center}
		\begin{lstlisting}[caption={Intrinsic Calibration Result}, language=c++, label={lst: intcalib}]
		image_width: 1624
		image_height: 1224
		camera_name: 0
		camera_matrix:
			rows: 3
			cols: 3
			data: [1454.423376687359, 0, 822.9545738617143, 0, 1458.005828758985, 590.5652711935882, 0, 0, 1]
		distortion_model: plumb_bob
		distortion_coefficients:
			rows: 1
			cols: 5
			data: [-0.2015966527847064, 0.1516937421259596, -0.0009340794635090795, -0.0006787308984611241, 0]
		rectification_matrix:
			rows: 3
			cols: 3
			data: [1, 0, 0, 0, 1, 0, 0, 0, 1]
		projection_matrix:
			rows: 3
			cols: 4
			data: [1379.264282226562, 0, 822.6802277325623, 0, 0, 1410.231689453125, 588.4764252277164, 0, 0, 0, 1, 0]\end{lstlisting}
	\end{center}
\end{figure}

In this file it is possible to see the image dimensions, the intrinsic values and distortions coefficients of the camera as well as the rectification and projection matrix. These values are used to reproject the points of the pointcloud into the camera's image. To do so, \gls{opencv} implements a method called \texttt{projectPoints} (\cite{OpenCVa}) that is used in the labelling node. 

Firstly, the labelling node reads the file in listing \ref{lst: intcalib} to calibrate the camera. Then, the full pointcloud is taken and a vector of the points is retrieved. The \texttt{projectPoints} method takes the calibration file parameters and the points vector and generates another vector with the $(x,y)$ coordinates of the points in the image.

The next step is simply to draw those points in the image using the function \texttt{circle} implemented by \gls{opencv}. The final result is as seen on the left in figure \ref{fig:projectpoints}.

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=.47\textwidth]{caplabel/imgs/projectpc.png}
	\includegraphics[width=.45\textwidth]{caplabel/imgs/rvizpc.png}
	
	\caption{Example of the pointcloud projection (left) in comparison with the Rviz 3D view (right)}
	\label{fig:projectpoints}
	
\end{figure}

With the pointclouds points projected in the image it is possible to associate the 3D clusters to the 2D image templates.


\subsection{Suggestion of Objects of Interest}

Using the algorithms previously developed it is possible to track objects in the 2D image using the position of the 3D clusters by picking up their points and following them.

A semi-automatic algorithm was developed where the tracking can be done manually by clicking while the semi-automatic system suggests objects of interest that may appear in the field of view. A simplified diagram of the algorithm is found in figure \ref{fig:bbdiagram}.

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=.6\textwidth]{caplabel/imgs/bbdiagram.pdf}
	
	\caption{Simplified diagram of the tracking with sensor fusion}
	\label{fig:bbdiagram}
	
\end{figure}

To begin, the full pointcloud data is gathered and passed into the \gls{mtt} algorithm. The \gls{mtt} library implements methods that handle detection and tracking of objects and outputs the coordinates of the found objects. 

A function polls the \gls{mtt} algorithm and raises a flag when an object is found. If more than an object is found, the nearest object is tracked. The coordinates of the objects relatively to the vehicle are found so the distances can be calculated. The object with shortest distance is chosen to track.

Having the coordinates of the tracked object, a cube with a fixed size is drawn centered in the point given by the \gls{mtt} algorithm by setting the cube vertices and using the \texttt{line} function implemented by \gls{opencv} to join them.

Then, the cube is projected to the image using \gls{opencv}'s \texttt{projectPoints}. Figure \ref{fig:cube} shows a 3D bounding box being projected around the detected car.

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=.6\textwidth]{caplabel/imgs/3dcube.png}
	
	\caption{Example of 3D bounding box tracking a car}
	\label{fig:cube}
	
\end{figure}

\subsection{Improvement of the Manual Labelling}

When the manual mode is active, the user selects the target to follow by clicking the image. In that location, the labelling node checks for an object found by the \gls{mtt} algorithm. 

\begin{figure}[h]
	
	\centering
	\includegraphics[width=.6\textwidth]{caplabel/imgs/mtt_proj.png}
	
	\caption{MTT exposing all found targets in the image}
	\label{fig:mtt_proj}
	
\end{figure}

All targets found by the \gls{mtt} are exposed in the image (see figure \ref{fig:mtt_proj}) and if the click is inside of one of the target's areas, the tracking is done using the \gls{mtt}. Otherwise, the tracking is done using template matching.

\section{Output Datasets}

\subsection{2D Dataset}

While tracking objects, the user is prompted to insert a label to the object. The user enters a class to which the object belongs to. The node saves several bounding boxes for each frame. To accomplish this, a bounding box data structure called \texttt{BBox} was implemented.

\begin{figure}
	\begin{center}
		\begin{lstlisting}[label={lst:BBox2ddef}, caption={BBox struct definition used for 2D datasets.},language=c++]
		struct BBox
		{
			int x;
			int y;
			int width;
			int height;
			int id;
			string label;
		};\end{lstlisting}
	\end{center}
\end{figure}

The \texttt{BBox} struct definition is presented in listing \ref{lst:BBox2ddef}. The \texttt{BBox} presents its $x$ and $y$ coordinates, its width and height, an id, and a label. While the tracking is performed, several instances of \texttt{BBox} are created and stored in a map that relates the frame with the \texttt{BBox}. 

When the tracking is complete, the user can opt to save the results or to discard them. If the frames are to be saved, the users enters the object label and a folder will be created with patches of the frames where the object appears. The user can also choose to label the objects without saving the templates. In the end, a set of \texttt{BBox} instances are created and a dataset file can be created.

\begin{figure}
	\begin{center}
		\begin{lstlisting}[label={lst:BBox2d_dataset}, caption={2D dataset example snippet},language=c++]
		FRAME_ID 
		BOX_X BOX_Y WIDTH HEIGHT LABEL ID
		...
		1083
		815 663 155 104 car 4
		1084
		816 662 155 104 car 4
		1142
		482 584 152 150 van 5
		1143
		512 589 152 150 van 5
		...\end{lstlisting}
	\end{center}
\end{figure}

The dataset contains, for each frame, a set of bounding boxes that are defined by their coordinates, size, label and object ID. The map where the set of \texttt{BBox} is stored is iterated and printed to a file similar to the snippet in listing \ref{lst:BBox2d_dataset}.


\subsection{3D Datasets}

To develop datasets and include the 3D information of the tracked targets, some changes have been made to the structure of the \texttt{BBox} (see listing \ref{lst:bbox3d}) in which the 3D coordinates of the objects have been added.

\begin{figure}
	\begin{center}
		\begin{lstlisting}[label={lst:bbox3d}, caption={BBox struct definition with 3D capabilities},language=c++]
		struct BBox
		{
			int x;
			int y;
			int width;
			int height;
			int id;
			string label;
			// 3D position
			double x3d, y3d, z3d;
		};		\end{lstlisting}
	\end{center}
\end{figure}

While tracking an object in the image, an object in the \gls{mtt} of ranged based sensors is selected and its ID is retrieved. This object is followed and its position is given to the labelling node to print a dataset file (see listing \ref{lst:dataset3d}) with the full information about the objects whereabouts.

\begin{figure}
	\begin{center}
		\begin{lstlisting}[label={lst:dataset3d}, caption={Snippet of the dataset with 3D capabilities},language=c++]
		FRAME_ID
		BOX_X BOX_Y WIDTH HEIGHT LABEL ID 3D_X 3D_Y 3D_Z
		...
		1063
		693 600 218 218 car 1 19.1706 1.64176 0.5
		1064
		692 597 218 218 car 1 19.5359 1.61985 0.5
		1144
		570 597 145 145 van 2 25.7349 2.61821 0.5
		1145
		590 602 145 145 van 2 25.7349 2.61821 0.5
		...		\end{lstlisting}
	\end{center}
\end{figure}

The dataset header was updated to contain the 3D information and the contents now present the coordinates in space of the object regarding the ATLASCAR 2 position. Analyzing the snippet in listing \ref{lst:dataset3d}, the \texttt{3D\_Z} values can be seen set to 0.5. The explanation for this is that the \gls{mtt} library implements perception for planar sensors which only give $x$ and $y$ coordinates. For this reason, the height for all objects is forced to 0.5 (half a meter). 

\section{UI Tools for Labelling}

In this section it will be described some extra tools and features implemented in addition to the detection, tracking and labelling of objects. Most features presented in this section are used to aid and complement what has been previously done.

\subsection{Labelling Interface}

The labelling node features a simplified \gls{gui} where the user can do several operations.

\begin{figure}[htp]
	\centering
	\includegraphics[width=.99\textwidth]{caplabel/imgs/labellinggui.png}
	\caption{Labelling Node \gls{gui}}
	\label{fig:labellinggui}	
\end{figure}

The \gls{gui} features an interactive window where the image sequences appear and the user may select targets. There is also a window with six buttons where multiple actions can be performed.

\begin{itemize}
\item \textbf{Label Object} - Pauses the rosbag and asks the user for a label of the target. Ends the tracking.
\item \textbf{Save Templates} - Pauses the rosbag and asks the user for a label of the target. Saves the image templates in all frames where the target appeared.
\item \textbf{Clear Image} - End the tracking and clears the bounding box from the image.
\item \textbf{Semi-Automatic/Manual Mode} - Switches between manual mode and semi automatic mode.
\item \textbf{Print Dataset} - Saves the dataset created during the whole labelling process.
\item \textbf{Quit} - Exits the program.
\end{itemize}

After successfully completing a full tracking, the user will be prompted with a window to enter a label for the target. The user may also discard the target instead of saving it.


\begin{figure}[htp]
	\centering
	\includegraphics[width=.4\textwidth]{caplabel/imgs/labellinggui2.png}
	\caption{Prompt window to select a label and save or discard.}
	\label{fig:labellinggui2}
\end{figure}

\subsection{Automatic Pause for the rqt\_bag}

A problem encountered during the labelling process was that the execution of the ROS bag file continued when the tracking was complete. The user needed to insert a label to the followed target and after the object is labelled the ROS bag continued creating a gap (see figure \ref{fig:problem}). 

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=0.7\textwidth]{caplabel/imgs/diagramtracking.pdf}
	
	\caption{A gap is created while the user is prompted to input a label because the ROS bag execution does not pause}
	\label{fig:problem}
	
\end{figure}

To solve this problem, the rqt\_bag package was adapted. A \gls{ros} service was implemented in the rqt\_bag node to receive messages from the outside. The labelling node sends a request to pause the rqt\_bag when the tracking ends. After the user inserts a label the labelling node sends a message to the rqt\_bag node to resume the bag execution.


\subsection{Playback}

An extra rosnode was developed with the aim to play the rosbag and identify the objects in the images using the dataset created previously while the rosbag plays in the background.

This rosnode starts similarly to the previous one, by subscribing to the camera \gls{ros} topic and send the image message to a callback function where it is processed. 

The message is converted into \gls{opencv} format and the dataset file is read. A map similar to the previous is also created to be filled with the dataset information where it relates the objects in the bounding boxes to the frames in the sequence. 

When a frame is received, its frame ID is retrieved. This ID is used to check which boxes in this frame. If this frame contains objects, a rectangle is draw in the image representing the bounding box acquired before. The box also features a legend with the object label and its ID. The color of the bounding box is randomly assigned, depending on its label. 

In other words, objects with the same label will have the same box color, making it easy to identify objects if the image has several different boxes. The resulting image is presented in figure \ref{fig:playback}. The image is then converted again into the \gls{ros} format and published to a topic.

\begin{figure}[htp]
	
	\centering
	\includegraphics[width=0.7\textwidth]{caplabel/imgs/playback.png}
	
	\caption{Playback example with a car}
	\label{fig:playback}
	
\end{figure}








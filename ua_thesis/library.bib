Automatically generated by Mendeley Desktop 1.19
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Sigal,
abstract = {While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of com-peting methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting HUMANEVA datasets contain multiple sub-jects performing a set of predefined actions with a number of repetitions. On the order of 40,000 frames of synchro-nized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60 Hz with an additional 37,000 time instants of pure mo-tion capture data. A standard set of error measures is defined This project was supported in part by gifts from Honda Research Institute and Intel Corporation. Funding for portions of this work was also provided by NSF grants IIS-0534858 and IIS-0535075. We would like to thank Ming-Hsuan Yang, Rui Li, Payman Yadollahpour and Stefan Roth for help in data collection and post-processing. We also would like to thank Stan Sclaroff for making the color video capture equipment available for this effort. for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively standard Bayesian framework with optimization in the form of Sequential Im-portance Resampling and Annealed Particle Filtering. In the context of this baseline algorithm we explore a vari-ety of likelihood functions, prior models of human motion and the effects of algorithm parameters. Our experiments suggest that image observation models and motion priors play important roles in performance, and that in a multi-view laboratory environment, where initialization is avail-able, Bayesian filtering tends to perform well. The datasets and the software are made available to the research com-munity. This infrastructure will support the development of new articulated motion and pose estimation algorithms, will provide a baseline for the evaluation and comparison of new methods, and will help establish the current state of the art in human pose estimation and tracking.},
author = {Sigal, Leonid and Balan, Alexandru O and Black, Michael J and Balan, A O and Black, M J},
doi = {10.1007/s11263-009-0273-6},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sigal et al. - Unknown - HUMANEVA Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Hum.pdf:pdf},
journal = {Int J Comput Vis},
keywords = {Articulated pose estimation {\textperiodcentered},Articulated tracking {\textperiodcentered},Datasets and evaluation,Human tracking {\textperiodcentered},Motion capture {\textperiodcentered}},
title = {{HUMANEVA: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion}},
url = {http://files.is.tue.mpg.de/black/papers/ehumIJCV10web.pdf},
year = {2009}
}
@article{Teo,
author = {Teo, Pauline},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teo - Unknown - SMART launches first Singapore-developed driverless car designed for operations on public roads.pdf:pdf},
title = {{SMART launches first Singapore-developed driverless car designed for operations on public roads}},
url = {https://smart.mit.edu/images/pdf/news/2014/Driverless{\_}Car{\_}NR{\_}270114{\_}Final.pdf},
year = {2018}
}
@misc{OpenCV2.4.13.6documentation2,
author = {{OpenCV 2.4.13.6 documentation}},
title = {{Template Matching}},
url = {https://docs.opencv.org/2.4/doc/tutorials/imgproc/histograms/template{\_}matching/template{\_}matching.html},
urldate = {2018-05-02},
year = {2018}
}
@misc{EPFLEcolepolytechniquefederaledeLausanne,
author = {{EPFL ({\'{E}}cole polytechnique f{\'{e}}d{\'{e}}rale de Lausanne)}},
title = {{Multi-camera pedestrians video | CVLAB}},
url = {https://cvlab.epfl.ch/data/pom},
urldate = {2018-04-18},
year = {2018}
}
@misc{SICK,
author = {SICK},
title = {{LMS151-10100 | Detection and ranging solutions | SICK}},
url = {https://www.sick.com/de/en/detection-and-ranging-solutions/2d-lidar-sensors/lms1xx/lms151-10100/p/p141840 https://www.sick.com/de/en/detection-and-ranging-solutions/3d-lidar-sensors/ld-mrs/ld-mrs400001/p/p112355},
urldate = {2018-04-20},
year = {2018}
}
@incollection{Spinello2010,
author = {Spinello, Luciano and Triebel, Rudolph and Siegwart, Roland},
doi = {10.1007/978-3-642-13408-1_12},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spinello, Triebel, Siegwart - 2010 - Multiclass Multimodal Detection and Tracking in Urban Environments ⋆.pdf:pdf},
pages = {125--135},
publisher = {Springer, Berlin, Heidelberg},
title = {{Multiclass Multimodal Detection and Tracking in Urban Environments}},
url = {http://link.springer.com/10.1007/978-3-642-13408-1{\_}12},
year = {2010}
}
@misc{PointCloudLibrary2018,
author = {{Point Cloud Library}},
title = {{About - Point Cloud Library (PCL)}},
url = {http://pointclouds.org/about/},
urldate = {2018-06-02},
year = {2018}
}
@misc{Team,
author = {{Boston Didi Team}},
title = {{KITTI Github Respository}},
url = {https://github.com/bostondiditeam/kitti},
year = {2018}
}
@misc{OpenCV2.4.13.6documentation,
author = {{OpenCV 2.4.13.6 documentation}},
title = {{Eroding and Dilating}},
url = {https://docs.opencv.org/2.4/doc/tutorials/imgproc/erosion{\_}dilatation/erosion{\_}dilatation.html},
urldate = {2018-04-29},
year = {2018}
}
@misc{ETHZEidgenossischeTechnischeHochschuleZurich,
author = {{ETHZ (Eidgen{\"{o}}ssische Technische Hochschule Z{\"{u}}rich)}},
title = {{Moving Obstacle Detection in Highly Dynamic Scenes}},
url = {https://data.vision.ee.ethz.ch/cvl/aess/dataset/},
urldate = {2018-04-18},
year = {2018}
}
@misc{OpenCVa,
author = {OpenCV},
title = {{Camera Calibration and 3D Reconstruction — OpenCV 2.4.13.6 documentation}},
url = {https://docs.opencv.org/2.4/modules/calib3d/doc/camera{\_}calibration{\_}and{\_}3d{\_}reconstruction.html},
urldate = {2018-06-05},
year = {2018}
}
@article{Correia2017,
author = {Correia, Jos{\'{e}}},
file = {:home/mikael/Desktop/TeseDiogoCorreia.pdf:pdf},
pages = {1--98},
title = {{Unidade de Perce{\c{c}}{\~{a}}o Visual e de profundidade para o ATLASCAR2}},
url = {http://lars.mec.ua.pt/public/LAR Projects/Perception/2017{\_}DiogoCorreia/TeseDiogoCorreia.pdf},
year = {2017}
}
@article{TheMilwaukeeSentinel,
author = {{The Milwaukee Sentinel}},
title = {{8 Dec 1926 - " 'Phantom Auto' will tour city"}},
url = {https://news.google.com/newspapers?id=unBQAAAAIBAJ{\&}sjid=QQ8EAAAAIBAJ{\&}pg=7304,3766749},
year = {1926}
}
@article{SoaresDeAlmeida2016a,
author = {{Soares De Almeida}, Jorge Manuel},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soares De Almeida - 2016 - Seguimento ativo de agentes din{\^{a}}micos multivariados usando informa{\c{c}}{\~{a}}o vectorial Active Tracking of Dyna(2).pdf:pdf},
title = {{Seguimento ativo de agentes din{\^{a}}micos multivariados usando informa{\c{c}}{\~{a}}o vectorial Active Tracking of Dynamic Multivariate Agents using Vectorial Range Data}},
url = {http://lars.mec.ua.pt/public/LAR Projects/Perception/2016{\_}JorgeAlmeida/Thesis/PhDThesis.pdf},
year = {2016}
}
@misc{MITSUBISHIMOTORS,
author = {{MITSUBISHI MOTORS}},
title = {{Specifications | i-MiEV}},
url = {https://www.mitsubishi-motors.com/en/showroom/i-miev/specifications/},
urldate = {2018-04-20},
year = {2018}
}
@misc{DARPA,
author = {DARPA},
title = {{DARPA Grand Challenge}},
url = {http://archive.darpa.mil/grandchallenge/},
urldate = {2018-05-29},
year = {2018}
}
@article{Geiger,
abstract = {—We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
author = {Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Geiger et al. - Unknown - Vision meets Robotics The KITTI Dataset.pdf:pdf},
keywords = {GPS,Index Terms—dataset,KITTI,SLAM,autonomous driving,benchmarks,cameras,computer vision,field robotics,laser,mobile robotics,object detection,optical flow,stereo,tracking},
title = {{Vision meets Robotics: The KITTI Dataset}},
url = {http://ww.cvlibs.net/publications/Geiger2013IJRR.pdf},
year = {2013}
}
@article{NancyHicks,
author = {{Nancy Hicks}},
title = {{Nebraska tested driverless car technology 60 years ago}},
url = {http://journalstar.com/news/local/govt-and-politics/nebraska-tested-driverless-car-technology-years-ago/article{\_}a702fab9-cac3-5a6e-a95c-9b597fdab078.html},
year = {2018}
}
@misc{AudiMediaCenter,
author = {{Audi MediaCenter}},
title = {{Audi at the IAA 2017: Autonomous driving in three steps}},
url = {https://www.audi-mediacenter.com/en/press-releases/audi-at-the-iaa-2017-autonomous-driving-in-three-steps-9311},
urldate = {2018-04-17},
year = {2018}
}
@misc{SoaresDeAlmeida,
author = {{Soares De Almeida}, Jorge Manuel},
title = {{Multi target tracking}},
url = {http://lars.mec.ua.pt/lartk/doc/mtt/html/},
year = {2018}
}
@misc{KarlsruheInstituteofTechnology,
author = {{Karlsruhe Institute of Technology}},
title = {{The KITTI Vision Benchmark Suite}},
url = {http://www.cvlibs.net/datasets/kitti/},
urldate = {2018-04-17},
year = {2018}
}
@article{Pomerleau1989,
abstract = {ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand.},
author = {Pomerleau, Dean A},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pomerleau - 1989 - An Autonomous Land Vehicle In a Neural Network.pdf:pdf},
title = {{An Autonomous Land Vehicle In a Neural Network}},
url = {http://repository.cmu.edu/cgi/viewcontent.cgi?article=2874{\&}context=compsci},
year = {1989}
}
@misc{SICKa,
author = {SICK},
title = {{LD-MRS400001 | Detection and ranging solutions | SICK}},
url = {https://www.sick.com/de/en/detection-and-ranging-solutions/3d-lidar-sensors/ld-mrs/ld-mrs400001/p/p112355},
urldate = {2018-04-20},
year = {2018}
}
@article{Suzuki1985,
abstract = {Two border following algorithms are proposed for the topological analysis of digitized binary images. The first one determines the surroundness relations among the borders of a binary image. Since the outer borders and the hole borders have a one-to-one correspondence to the connected components of 1-pixels and to the holes, respectively, the proposed algorithm yields a representation of a binary image, from which one can extract some sort of features without reconstructing the image. The second algorithm, which is a modified version of the first, follows only the outermost borders (i.e., the outer borders which are not surrounded by holes). These algorithms can be effectively used in component counting, shrinking, and topological structural analysis of binary images, when a sequential digital computer is used. {\textcopyright} 1985.},
author = {Suzuki, Satoshi and Abe, Keiichi A.},
doi = {10.1016/0734-189X(85)90016-7},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/suzuki1985.pdf:pdf},
isbn = {0734-189X},
issn = {0734189X},
journal = {Computer Vision, Graphics and Image Processing},
number = {1},
pages = {32--46},
title = {{Topological structural analysis of digitized binary images by border following}},
volume = {30},
year = {1985}
}
@misc{PointGrey,
author = {PointGrey},
title = {{Zebra2 2.0 MP Color GigE / HD-SDI (Sony ICX274)}},
url = {https://www.ptgrey.com/zebra2-2-mp-color-gige-vision-hd-sdi-sony-icx274-camera},
urldate = {2018-04-20},
year = {2018}
}
@incollection{Kroger2016,
address = {Berlin, Heidelberg},
author = {Kr{\"{o}}ger, Fabian},
booktitle = {Autonomous Driving},
doi = {10.1007/978-3-662-48847-8_3},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kr{\"{o}}ger - 2016 - Automated Driving in Its Social, Historical and Cultural Contexts.pdf:pdf},
pages = {41--68},
publisher = {Springer Berlin Heidelberg},
title = {{Automated Driving in Its Social, Historical and Cultural Contexts}},
url = {http://link.springer.com/10.1007/978-3-662-48847-8{\_}3},
year = {2016}
}
@misc{OpenCV,
author = {OpenCV},
title = {{Background Subtraction}},
url = {https://docs.opencv.org/3.1.0/db/d5c/tutorial{\_}py{\_}bg{\_}subtraction.html},
urldate = {2018-04-29},
year = {2018}
}
@misc{Waymo,
author = {Waymo},
title = {{Waymo}},
url = {https://waymo.com/},
urldate = {2018-04-17},
year = {2018}
}
@misc{OpenCV2.4.13.6documentationa,
author = {{OpenCV 2.4.13.6 documentation}},
title = {{Canny Edge Detector}},
url = {https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/canny{\_}detector/canny{\_}detector.html},
urldate = {2018-04-30},
year = {2018}
}
@misc{OpenCV2.4.13.6documentationb,
author = {{OpenCV 2.4.13.6 documentation}},
title = {{Sobel Derivatives}},
url = {https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/sobel{\_}derivatives/sobel{\_}derivatives.html},
urldate = {2018-04-30},
year = {2018}
}
@misc{ROSWikia,
author = {{ROS Wiki}},
title = {rosbag},
url = {http://wiki.ros.org/rosbag},
urldate = {2018-05-05},
year = {2018}
}
@incollection{Kroger2016,
address = {Berlin, Heidelberg},
author = {Kr{\"{o}}ger, Fabian},
booktitle = {Autonomous Driving},
doi = {10.1007/978-3-662-48847-8_3},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kr{\"{o}}ger - 2016 - Automated Driving in Its Social, Historical and Cultural Contexts.pdf:pdf},
pages = {41--68},
publisher = {Springer Berlin Heidelberg},
title = {{Automated Driving in Its Social, Historical and Cultural Contexts}},
url = {http://link.springer.com/10.1007/978-3-662-48847-8{\_}3},
year = {2016}
}
@article{VieiradaSilva2016,
abstract = {The work presented in this thesis expands an existing extrinsic calibration$\backslash$npackage to vision-based sensors. A ball is used as a calibration target to$\backslash$nestimate the pose of multiple sensors relative to a reference frame. First, an$\backslash$nalgorithm is developed to detect the ball in an image then, two methods are$\backslash$nimplemented and tested to estimate the camera pose relative to a reference$\backslash$nframe. Additionally, a graphical user interface is developed for the extrinsic$\backslash$ncalibration package. The interface allows multiple sensor configurations to$\backslash$nbe calibrated, greatly simplifies the calibration procedure and is easily expandable$\backslash$nto new sensors. To illustrate the expansion process, Microsoft$\backslash$nKinect 3D-depth sensor is integrated in the calibration package and interface.$\backslash$nFinally, from the estimated poses, sensor data fusion is achieved with$\backslash$nATLASCAR 1, an autonomous vehicle, and in indoors tests.},
author = {{Vieira da Silva}, David Tiago},
file = {:home/mikael/Desktop/Tese{\_}David{\_}Silva{\_}65033.pdf:pdf},
pages = {107},
title = {{Multisensor Calibration and Data Fusion Using LIDAR and Vision}},
url = {http://lars.mec.ua.pt/public/LAR Projects/Perception/2016{\_}DavidSilva/Tese{\_}David{\_}Silva{\_}65033.pdf},
year = {2016}
}
@misc{Singapore-MITAllianceforResearchandTechnology,
author = {{Singapore-MIT Alliance for Research and Technology}},
title = {{SMART - Singapore-MIT Alliance for Research and Technology}},
url = {https://smart.mit.edu/},
urldate = {2018-04-17},
year = {2018}
}
@article{Andriluka2018,
abstract = {Automatic recovery of 3D human pose from monocular image sequences is a challenging and important research topic with numerous applications. Although current meth-ods are able to recover 3D pose for a single person in con-trolled environments, they are severely challenged by real-world scenarios, such as crowded street scenes. To address this problem, we propose a three-stage process building on a number of recent advances. The first stage obtains an ini-tial estimate of the 2D articulation and viewpoint of the per-son from single frames. The second stage allows early data association across frames based on tracking-by-detection. These two stages successfully accumulate the available 2D image evidence into robust estimates of 2D limb positions over short image sequences (= tracklets). The third and final stage uses those tracklet-based estimates as robust im-age observations to reliably recover 3D pose. We demon-strate state-of-the-art performance on the HumanEva II benchmark, and also show the applicability of our approach to articulated 3D tracking in realistic street conditions.},
author = {Andriluka, Mykhaylo and Roth, Stefan and Schiele, Bernt},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andriluka, Roth, Schiele - Unknown - Monocular 3D Pose Estimation and Tracking by Detection.pdf:pdf},
title = {{Monocular 3D Pose Estimation and Tracking by Detection}},
url = {https://pdfs.semanticscholar.org/f042/e85c26cd3638fcdc6599aa546d85045a7c5d.pdf},
year = {2018}
}
@misc{ROSWiki,
author = {{ROS Wiki}},
title = {rviz},
url = {http://wiki.ros.org/rviz},
urldate = {2018-05-05},
year = {2018}
}
@misc{LARlabs,
abstract = {ATLAS is a project created by the Group of Automation and Robotics at the Department of Mechanical Engineering of the University of Aveiro, Portugal. The mission of the ATLAS project is to develop and enable the proliferation of advanced sensing and active systems designed for implementation in automobiles and affine platforms. Advanced active systems being improved, or newly developed, use data from vision, laser and other sensors. The ATLAS project has vast experience with autonomous navigation in controlled environments and is now evolving to deal with real road scenarios. To ensure that the developments are meeting the ATLAS project mission statement, a full sized prototype, the ATLASCAR 1, has been equipped with several state of the art sensors.},
author = {LARlabs},
title = {{ATLAS project}},
url = {http://atlas.web.ua.pt/},
year = {2018}
}

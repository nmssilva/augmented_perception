Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{SoaresDeAlmeida,
author = {{Soares De Almeida}, Jorge Manuel},
title = {{Multi target tracking}},
url = {http://lars.mec.ua.pt/lartk/doc/mtt/html/}
}
@misc{MITSUBISHIMOTORS,
author = {{MITSUBISHI MOTORS}},
title = {{Specifications | i-MiEV}},
url = {https://www.mitsubishi-motors.com/en/showroom/i-miev/specifications/},
urldate = {2018-04-20}
}
@misc{OpenCV2.4.13.6documentation,
author = {{OpenCV 2.4.13.6 documentation}},
title = {{Eroding and Dilating}},
url = {https://docs.opencv.org/2.4/doc/tutorials/imgproc/erosion{\_}dilatation/erosion{\_}dilatation.html},
urldate = {2018-04-29}
}
@article{Correia2017,
author = {Correia, Jos{\'{e}}},
file = {:home/mikael/Desktop/TeseDiogoCorreia.pdf:pdf},
pages = {1--98},
title = {{Unidade de Perce{\c{c}}{\~{a}}o Visual e de profundidade para o ATLASCAR2}},
url = {http://lars.mec.ua.pt/public/LAR Projects/Perception/2017{\_}DiogoCorreia/TeseDiogoCorreia.pdf},
year = {2017}
}
@article{Teo,
author = {Teo, Pauline},
file = {::},
title = {{SMART launches first Singapore-developed driverless car designed for operations on public roads}},
url = {https://smart.mit.edu/images/pdf/news/2014/Driverless{\_}Car{\_}NR{\_}270114{\_}Final.pdf}
}
@article{VieiradaSilva2016,
abstract = {The work presented in this thesis expands an existing extrinsic calibration$\backslash$npackage to vision-based sensors. A ball is used as a calibration target to$\backslash$nestimate the pose of multiple sensors relative to a reference frame. First, an$\backslash$nalgorithm is developed to detect the ball in an image then, two methods are$\backslash$nimplemented and tested to estimate the camera pose relative to a reference$\backslash$nframe. Additionally, a graphical user interface is developed for the extrinsic$\backslash$ncalibration package. The interface allows multiple sensor configurations to$\backslash$nbe calibrated, greatly simplifies the calibration procedure and is easily expandable$\backslash$nto new sensors. To illustrate the expansion process, Microsoft$\backslash$nKinect 3D-depth sensor is integrated in the calibration package and interface.$\backslash$nFinally, from the estimated poses, sensor data fusion is achieved with$\backslash$nATLASCAR 1, an autonomous vehicle, and in indoors tests.},
author = {{Vieira da Silva}, David Tiago},
file = {:home/mikael/Desktop/Tese{\_}David{\_}Silva{\_}65033.pdf:pdf},
pages = {107},
title = {{Multisensor Calibration and Data Fusion Using LIDAR and Vision}},
url = {http://lars.mec.ua.pt/public/LAR Projects/Perception/2016{\_}DavidSilva/Tese{\_}David{\_}Silva{\_}65033.pdf},
year = {2016}
}
@article{TheMilwaukeeSentinel,
author = {{The Milwaukee Sentinel}},
title = {{8 Dec 1926 - " 'Phantom Auto' will tour city"}},
url = {https://news.google.com/newspapers?id=unBQAAAAIBAJ{\&}sjid=QQ8EAAAAIBAJ{\&}pg=7304,3766749}
}
@misc{Team,
author = {Team, Boston Didi},
title = {{KITTI Github Respository}},
url = {https://github.com/bostondiditeam/kitti}
}
@misc{OpenCV2.4.13.6documentationa,
author = {{OpenCV 2.4.13.6 documentation}},
title = {{Canny Edge Detector}},
url = {https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/canny{\_}detector/canny{\_}detector.html},
urldate = {2018-04-30}
}
@incollection{Kroger2016,
address = {Berlin, Heidelberg},
author = {Kr{\"{o}}ger, Fabian},
booktitle = {Autonomous Driving},
doi = {10.1007/978-3-662-48847-8_3},
file = {::},
pages = {41--68},
publisher = {Springer Berlin Heidelberg},
title = {{Automated Driving in Its Social, Historical and Cultural Contexts}},
url = {http://link.springer.com/10.1007/978-3-662-48847-8{\_}3},
year = {2016}
}
@misc{SICKa,
author = {SICK},
title = {{LD-MRS400001 | Detection and ranging solutions | SICK}},
url = {https://www.sick.com/de/en/detection-and-ranging-solutions/3d-lidar-sensors/ld-mrs/ld-mrs400001/p/p112355},
urldate = {2018-04-20}
}
@misc{Singapore-MITAllianceforResearchandTechnology,
author = {{Singapore-MIT Alliance for Research and Technology}},
title = {{SMART - Singapore-MIT Alliance for Research and Technology}},
url = {https://smart.mit.edu/},
urldate = {2018-04-17}
}
@misc{PointGrey,
author = {PointGrey},
title = {{Zebra2 2.0 MP Color GigE / HD-SDI (Sony ICX274)}},
url = {https://www.ptgrey.com/zebra2-2-mp-color-gige-vision-hd-sdi-sony-icx274-camera},
urldate = {2018-04-20}
}
@misc{LARlabs,
abstract = {ATLAS is a project created by the Group of Automation and Robotics at the Department of Mechanical Engineering of the University of Aveiro, Portugal. The mission of the ATLAS project is to develop and enable the proliferation of advanced sensing and active systems designed for implementation in automobiles and affine platforms. Advanced active systems being improved, or newly developed, use data from vision, laser and other sensors. The ATLAS project has vast experience with autonomous navigation in controlled environments and is now evolving to deal with real road scenarios. To ensure that the developments are meeting the ATLAS project mission statement, a full sized prototype, the ATLASCAR 1, has been equipped with several state of the art sensors.},
author = {LARlabs},
title = {{ATLAS project}},
url = {http://atlas.web.ua.pt/}
}
@misc{ETHZEidgenossischeTechnischeHochschuleZurich,
author = {{ETHZ (Eidgen{\"{o}}ssische Technische Hochschule Z{\"{u}}rich)}},
title = {{Moving Obstacle Detection in Highly Dynamic Scenes}},
url = {https://data.vision.ee.ethz.ch/cvl/aess/dataset/},
urldate = {2018-04-18}
}
@article{SoaresDeAlmeida2016a,
author = {{Soares De Almeida}, Jorge Manuel},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soares De Almeida - 2016 - Seguimento ativo de agentes din{\^{a}}micos multivariados usando informa{\c{c}}{\~{a}}o vectorial Active Tracking of Dyna(2).pdf:pdf},
title = {{Seguimento ativo de agentes din{\^{a}}micos multivariados usando informa{\c{c}}{\~{a}}o vectorial Active Tracking of Dynamic Multivariate Agents using Vectorial Range Data}},
url = {http://lars.mec.ua.pt/public/LAR Projects/Perception/2016{\_}JorgeAlmeida/Thesis/PhDThesis.pdf},
year = {2016}
}
@article{Suzuki1985,
abstract = {Two border following algorithms are proposed for the topological analysis of digitized binary images. The first one determines the surroundness relations among the borders of a binary image. Since the outer borders and the hole borders have a one-to-one correspondence to the connected components of 1-pixels and to the holes, respectively, the proposed algorithm yields a representation of a binary image, from which one can extract some sort of features without reconstructing the image. The second algorithm, which is a modified version of the first, follows only the outermost borders (i.e., the outer borders which are not surrounded by holes). These algorithms can be effectively used in component counting, shrinking, and topological structural analysis of binary images, when a sequential digital computer is used. {\textcopyright} 1985.},
author = {Suzuki, Satoshi and Be, Keiichi A.},
doi = {10.1016/0734-189X(85)90016-7},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/suzuki1985.pdf:pdf},
isbn = {0734-189X},
issn = {0734189X},
journal = {Computer Vision, Graphics and Image Processing},
number = {1},
pages = {32--46},
title = {{Topological structural analysis of digitized binary images by border following}},
volume = {30},
year = {1985}
}
@misc{KarlsruheInstituteofTechnology,
author = {{Karlsruhe Institute of Technology}},
title = {{The KITTI Vision Benchmark Suite}},
url = {http://www.cvlibs.net/datasets/kitti/},
urldate = {2018-04-17}
}
@misc{EPFLEcolepolytechniquefederaledeLausanne,
author = {{EPFL ({\'{E}}cole polytechnique f{\'{e}}d{\'{e}}rale de Lausanne)}},
title = {{Multi-camera pedestrians video | CVLAB}},
url = {https://cvlab.epfl.ch/data/pom},
urldate = {2018-04-18}
}
@misc{SICK,
author = {SICK},
title = {{LMS151-10100 | Detection and ranging solutions | SICK}},
url = {https://www.sick.com/de/en/detection-and-ranging-solutions/2d-lidar-sensors/lms1xx/lms151-10100/p/p141840},
urldate = {2018-04-20}
}
@misc{AudiMediaCenter,
author = {{Audi MediaCenter}},
title = {{Audi at the IAA 2017: Autonomous driving in three steps}},
url = {https://www.audi-mediacenter.com/en/press-releases/audi-at-the-iaa-2017-autonomous-driving-in-three-steps-9311},
urldate = {2018-04-17}
}
@article{Pomerleau1989,
abstract = {ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand.},
author = {Pomerleau, Dean A},
file = {::},
title = {{An Autonomous Land Vehicle In a Neural Network}},
url = {http://repository.cmu.edu/cgi/viewcontent.cgi?article=2874{\&}context=compsci},
year = {1989}
}
@misc{OpenCV2.4.13.6documentationb,
author = {{OpenCV 2.4.13.6 documentation}},
title = {{Sobel Derivatives}},
url = {https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/sobel{\_}derivatives/sobel{\_}derivatives.html},
urldate = {2018-04-30}
}
@incollection{Spinello2010,
author = {Spinello, Luciano and Triebel, Rudolph and Siegwart, Roland},
doi = {10.1007/978-3-642-13408-1_12},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spinello, Triebel, Siegwart - 2010 - Multiclass Multimodal Detection and Tracking in Urban Environments ⋆.pdf:pdf},
pages = {125--135},
publisher = {Springer, Berlin, Heidelberg},
title = {{Multiclass Multimodal Detection and Tracking in Urban Environments}},
url = {http://link.springer.com/10.1007/978-3-642-13408-1{\_}12},
year = {2010}
}
@article{Geiger,
abstract = {—We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
author = {Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Geiger et al. - Unknown - Vision meets Robotics The KITTI Dataset.pdf:pdf},
keywords = {GPS,Index Terms—dataset,KITTI,SLAM,autonomous driving,benchmarks,cameras,computer vision,field robotics,laser,mobile robotics,object detection,optical flow,stereo,tracking},
title = {{Vision meets Robotics: The KITTI Dataset}},
url = {http://ww.cvlibs.net/publications/Geiger2013IJRR.pdf}
}
@misc{Waymo,
author = {Waymo},
title = {{Waymo}},
url = {https://waymo.com/},
urldate = {2018-04-17}
}
@misc{OpenCV,
author = {OpenCV},
title = {{Background Subtraction}},
url = {https://docs.opencv.org/3.1.0/db/d5c/tutorial{\_}py{\_}bg{\_}subtraction.html},
urldate = {2018-04-29}
}
@article{Sigal,
abstract = {While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of com-peting methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting HUMANEVA datasets contain multiple sub-jects performing a set of predefined actions with a number of repetitions. On the order of 40,000 frames of synchro-nized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60 Hz with an additional 37,000 time instants of pure mo-tion capture data. A standard set of error measures is defined This project was supported in part by gifts from Honda Research Institute and Intel Corporation. Funding for portions of this work was also provided by NSF grants IIS-0534858 and IIS-0535075. We would like to thank Ming-Hsuan Yang, Rui Li, Payman Yadollahpour and Stefan Roth for help in data collection and post-processing. We also would like to thank Stan Sclaroff for making the color video capture equipment available for this effort. for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively standard Bayesian framework with optimization in the form of Sequential Im-portance Resampling and Annealed Particle Filtering. In the context of this baseline algorithm we explore a vari-ety of likelihood functions, prior models of human motion and the effects of algorithm parameters. Our experiments suggest that image observation models and motion priors play important roles in performance, and that in a multi-view laboratory environment, where initialization is avail-able, Bayesian filtering tends to perform well. The datasets and the software are made available to the research com-munity. This infrastructure will support the development of new articulated motion and pose estimation algorithms, will provide a baseline for the evaluation and comparison of new methods, and will help establish the current state of the art in human pose estimation and tracking.},
author = {Sigal, Leonid and Balan, Alexandru O and Black, Michael J and Balan, A O and Black, M J},
doi = {10.1007/s11263-009-0273-6},
file = {:home/mikael/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sigal et al. - Unknown - HUMANEVA Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Hum.pdf:pdf},
journal = {Int J Comput Vis},
keywords = {Articulated pose estimation {\textperiodcentered},Articulated tracking {\textperiodcentered},Datasets and evaluation,Human tracking {\textperiodcentered},Motion capture {\textperiodcentered}},
title = {{HUMANEVA: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion}},
url = {http://files.is.tue.mpg.de/black/papers/ehumIJCV10web.pdf}
}
@article{NancyHicks,
author = {{Nancy Hicks}},
title = {{Nebraska tested driverless car technology 60 years ago}},
url = {http://journalstar.com/news/local/govt-and-politics/nebraska-tested-driverless-car-technology-years-ago/article{\_}a702fab9-cac3-5a6e-a95c-9b597fdab078.html}
}
